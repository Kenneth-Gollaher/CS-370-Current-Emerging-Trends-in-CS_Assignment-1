{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 9s 189us/step - loss: 1.5284 - accuracy: 0.6085 - val_loss: 0.7963 - val_accuracy: 0.8273\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 8s 167us/step - loss: 0.6185 - accuracy: 0.8482 - val_loss: 0.4710 - val_accuracy: 0.8793\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 9s 180us/step - loss: 0.4466 - accuracy: 0.8826 - val_loss: 0.3841 - val_accuracy: 0.8980\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 7s 142us/step - loss: 0.3823 - accuracy: 0.8958 - val_loss: 0.3427 - val_accuracy: 0.9046\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.3465 - accuracy: 0.9034 - val_loss: 0.3163 - val_accuracy: 0.9124\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.3222 - accuracy: 0.9090 - val_loss: 0.2980 - val_accuracy: 0.9168\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 4s 73us/step - loss: 0.3035 - accuracy: 0.9145 - val_loss: 0.2823 - val_accuracy: 0.9208\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.2885 - accuracy: 0.9181 - val_loss: 0.2699 - val_accuracy: 0.9246\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 4s 82us/step - loss: 0.2761 - accuracy: 0.9208 - val_loss: 0.2596 - val_accuracy: 0.9277\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.2650 - accuracy: 0.9244 - val_loss: 0.2515 - val_accuracy: 0.9281\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 5s 114us/step - loss: 0.2550 - accuracy: 0.9276 - val_loss: 0.2428 - val_accuracy: 0.9310\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 8s 172us/step - loss: 0.2459 - accuracy: 0.9299 - val_loss: 0.2354 - val_accuracy: 0.9334\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 8s 174us/step - loss: 0.2377 - accuracy: 0.9325 - val_loss: 0.2315 - val_accuracy: 0.9343\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 8s 172us/step - loss: 0.2305 - accuracy: 0.9346 - val_loss: 0.2236 - val_accuracy: 0.9370\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 9s 185us/step - loss: 0.2233 - accuracy: 0.9366 - val_loss: 0.2170 - val_accuracy: 0.9383\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 8s 173us/step - loss: 0.2168 - accuracy: 0.9381 - val_loss: 0.2115 - val_accuracy: 0.9415\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 8s 174us/step - loss: 0.2107 - accuracy: 0.9400 - val_loss: 0.2064 - val_accuracy: 0.9427\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 8s 175us/step - loss: 0.2046 - accuracy: 0.9413 - val_loss: 0.2018 - val_accuracy: 0.9439\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 9s 180us/step - loss: 0.1990 - accuracy: 0.9434 - val_loss: 0.1978 - val_accuracy: 0.9448\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 9s 185us/step - loss: 0.1939 - accuracy: 0.9449 - val_loss: 0.1931 - val_accuracy: 0.9469\n",
      "10000/10000 [==============================] - 1s 127us/step\n",
      "Test score: 0.19546455649137498\n",
      "Test accuracy: 0.9422000050544739\n"
     ]
    }
   ],
   "source": [
    "#Test 1: Original from Deep Learning with Keras Book.\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility \n",
    "\n",
    "# network and training \n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10       # number of outputs = number of digits\n",
    "OPTIMIZER = SGD()     # SGD optimizer\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2  # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "# \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "# \n",
    "X_train /= 255\n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "#\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "#\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "#\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Test \n",
    "In the Deep Learning with Keras book, pages 22-24 I added two hidden layers and modified the iterations from 200, to 20. The optimizer tries to adjust the weights so that the objective function is minimised. The test accuracy is listed below:\n",
    "##### Training Set: 94.49%\n",
    "##### Validation: 94.69%\n",
    "##### Test Accuracy: 94.22%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 1.9120 - accuracy: 0.4837 - val_loss: 1.3933 - val_accuracy: 0.7206\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 1.0512 - accuracy: 0.7660 - val_loss: 0.7542 - val_accuracy: 0.8268\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.6728 - accuracy: 0.8336 - val_loss: 0.5460 - val_accuracy: 0.8639\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.5314 - accuracy: 0.8611 - val_loss: 0.4543 - val_accuracy: 0.8832\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.4596 - accuracy: 0.8763 - val_loss: 0.4027 - val_accuracy: 0.8964\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.4159 - accuracy: 0.8860 - val_loss: 0.3701 - val_accuracy: 0.9018\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.3860 - accuracy: 0.8930 - val_loss: 0.3465 - val_accuracy: 0.9084\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.3639 - accuracy: 0.8972 - val_loss: 0.3294 - val_accuracy: 0.9093\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.3470 - accuracy: 0.9011 - val_loss: 0.3159 - val_accuracy: 0.9120\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.3330 - accuracy: 0.9046 - val_loss: 0.3050 - val_accuracy: 0.9144\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.3211 - accuracy: 0.9077 - val_loss: 0.2950 - val_accuracy: 0.9169\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.3107 - accuracy: 0.9107 - val_loss: 0.2867 - val_accuracy: 0.9187\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.3015 - accuracy: 0.9139 - val_loss: 0.2803 - val_accuracy: 0.9202\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.2935 - accuracy: 0.9150 - val_loss: 0.2725 - val_accuracy: 0.9221\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.2860 - accuracy: 0.9181 - val_loss: 0.2659 - val_accuracy: 0.9245\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.2791 - accuracy: 0.9196 - val_loss: 0.2603 - val_accuracy: 0.9267\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.2727 - accuracy: 0.9211 - val_loss: 0.2554 - val_accuracy: 0.9274\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.2666 - accuracy: 0.9237 - val_loss: 0.2501 - val_accuracy: 0.9283\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.2610 - accuracy: 0.9253 - val_loss: 0.2455 - val_accuracy: 0.9306\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.2558 - accuracy: 0.9268 - val_loss: 0.2413 - val_accuracy: 0.9308\n",
      "10000/10000 [==============================] - 0s 36us/step\n",
      "Test score: 0.24487752861380577\n",
      "Test accuracy: 0.9318000078201294\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Modified BATCH_SIZE = 256.\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility \n",
    "\n",
    "# network and training \n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 256      # number of training instances observed before optimizer performs a weight update\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10       # number of outputs = number of digits\n",
    "OPTIMIZER = SGD()     # SGD optimizer\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2  # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "# \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "# \n",
    "X_train /= 255\n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "#\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "#\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "#\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Modified BATCH_SIZE = 256\n",
    "Here you can see the test accuracy decreasing with the batch size increasing. The iterations remained the same at 20. The new batch size is set at 256. There is a test accuracy loss comparing the original test with the batch size set to 128. Test results below:\n",
    "##### Training Set: 92.68%\n",
    "##### Validation: 93.08%\n",
    "##### Test Accuracy: 93.18%\n",
    "##### Loss Difference: 1.1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 2.1706 - accuracy: 0.2802 - val_loss: 1.9534 - val_accuracy: 0.5114\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 1.7212 - accuracy: 0.6230 - val_loss: 1.4388 - val_accuracy: 0.7128\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 1.2298 - accuracy: 0.7387 - val_loss: 0.9995 - val_accuracy: 0.7862\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.8993 - accuracy: 0.7942 - val_loss: 0.7554 - val_accuracy: 0.8295\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.7179 - accuracy: 0.8282 - val_loss: 0.6206 - val_accuracy: 0.8536\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.6112 - accuracy: 0.8484 - val_loss: 0.5381 - val_accuracy: 0.8671\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.5428 - accuracy: 0.8624 - val_loss: 0.4834 - val_accuracy: 0.8794\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.4955 - accuracy: 0.8718 - val_loss: 0.4457 - val_accuracy: 0.8868\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.4613 - accuracy: 0.8785 - val_loss: 0.4177 - val_accuracy: 0.8917ccuracy: 0.\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.4352 - accuracy: 0.8836 - val_loss: 0.3963 - val_accuracy: 0.8955\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.4147 - accuracy: 0.8871 - val_loss: 0.3790 - val_accuracy: 0.8988\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.3979 - accuracy: 0.8913 - val_loss: 0.3653 - val_accuracy: 0.9012\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.3840 - accuracy: 0.8946 - val_loss: 0.3542 - val_accuracy: 0.9042\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.3721 - accuracy: 0.8971 - val_loss: 0.3437 - val_accuracy: 0.9059\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.3619 - accuracy: 0.8993 - val_loss: 0.3354 - val_accuracy: 0.9076\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.3529 - accuracy: 0.9014 - val_loss: 0.3276 - val_accuracy: 0.9107\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.3449 - accuracy: 0.9033 - val_loss: 0.3208 - val_accuracy: 0.9120\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.3376 - accuracy: 0.9055 - val_loss: 0.3151 - val_accuracy: 0.9133\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.3311 - accuracy: 0.9072 - val_loss: 0.3094 - val_accuracy: 0.9154\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.3251 - accuracy: 0.9089 - val_loss: 0.3044 - val_accuracy: 0.9159\n",
      "10000/10000 [==============================] - 0s 37us/step\n",
      "Test score: 0.3068924264788628\n",
      "Test accuracy: 0.9132000207901001\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Modified BATCH_SIZE = 512\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility \n",
    "\n",
    "# network and training \n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 512      # number of training instances observed before optimizer performs a weight update\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10       # number of outputs = number of digits\n",
    "OPTIMIZER = SGD()     # SGD optimizer\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2  # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "# \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "# \n",
    "X_train /= 255\n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "#\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "#\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "#\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Modified BATCH_SIZE = 512\n",
    "Here you can see the test accuracy decreasing with the batch size increasing. The iterations remained the same at 20. The new batch size is set at 512. There is a test accuracy loss comparing the original test with a batch size of 128 as well as the previous batch size of 256. Test results below:\n",
    "\n",
    "##### Training Set: 90.89%\n",
    "##### Validation: 91.59%\n",
    "##### Test Accuracy: 91.32%\n",
    "##### Loss Difference from Batch Size 128: 3.1%\n",
    "##### Loss Difference from Batch Size 256: 2.0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above tests, various batch sizes were tested starting with 128, then to 256, and lastly to 512. As the batch size increases, the test accuracy shows a decent. The batch size can also have a significant impact on your model’s performance and the training time. A small batch size ensures that each training iteration is very fast, and a larger batch size will give a more precise estimate of the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
